# Stream_Vits_Zh

Stream_Vits_Zh æ˜¯ä¸€ä¸ªåŸºäº **VITS** çš„æµå¼ TTS æ–¹æ¡ˆï¼Œé€‚ç”¨äºéœ€è¦**ä½å»¶è¿Ÿ**çš„è¯­éŸ³åˆæˆåº”ç”¨ã€‚

æœ¬é¡¹ç›®åŸºäº [vits_chinese](https://github.com/PlayVoice/vits_chinese)ï¼Œæä¾›äº†ä¸€ä¸ªæ”¯æŒ**æµå¼è¾“å‡º**çš„ä¸­æ–‡ VITS æ¨¡å‹æ¥å£ï¼Œä¸“ä¸º **ä¸­æ–‡ TTS** ä»»åŠ¡ä¼˜åŒ–ã€‚

## **âœ¨ ç‰¹æ€§**

âœ… **åŸºäº VITS_chinese**ï¼šä¸“ä¸ºä¸­æ–‡è¯­éŸ³åˆæˆä¼˜åŒ–ï¼Œæä¾›é«˜è´¨é‡çš„ TTS è¾“å‡ºã€‚

âœ… **æ”¯æŒæµå¼è¾“å‡º**ï¼šä½å»¶è¿Ÿã€é€æ­¥ç”ŸæˆéŸ³é¢‘ï¼Œæé«˜å®æ—¶æ€§ã€‚

âœ… **å…¼å®¹ Coqui-TTS**ï¼šå¯ä½œä¸º **Coqui-TTS** çš„ä¸­æ–‡æµå¼æ¨¡å‹è¡¥å……ï¼Œæä¾›æ›´å¥½çš„æµå¼ TTS æ€§èƒ½ã€‚

âœ… **æ˜“äºéƒ¨ç½²**ï¼šç®€æ´çš„ API è®¾è®¡ï¼Œæ–¹ä¾¿é›†æˆï¼Œé€‚ç”¨äºå„ç§åº”ç”¨åœºæ™¯ã€‚

---

## **ğŸš€ åœ¨çº¿ Demo**

https://github.com/user-attachments/assets/788da5c5-ff62-4e09-bdfd-e33c240e1cda

---

## **ğŸ“¦ å®‰è£…ä¾èµ–**

### **1ï¸âƒ£ å…‹éš†é¡¹ç›®**
```bash
git clone https://github.com/zzl410/stream_vits_zh.git
cd stream_vits_zh
```

### **2ï¸âƒ£ å®‰è£…ä¾èµ–**
```bash
pip install -r requirements.txt
```

âš ï¸ **æ¨è Python 3.8+ ç‰ˆæœ¬**

### **3ï¸âƒ£ å®‰è£… `monotonic_align` ä¾èµ–**
```bash
cd monotonic_align
cythonize -i core.pyx
cd ..
```

---

## **ğŸ“¥ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†**

### **1ï¸âƒ£ ä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹**

ğŸ”— [vits_chinese/releases](https://github.com/zzl410/stream_vits_zh/releases/tag/v1.0)

å°† **`prosody_model.pt`** æ”¾å…¥ `./bert/prosody_model.pt`

å°† **`vits_bert_model.pth`** æ”¾å…¥ `./vits_bert_model.pth`

### **2ï¸âƒ£ è¿è¡Œæ¨ç†**
```bash
python stream_example.py
```

ç”Ÿæˆçš„éŸ³é¢‘å—å°†**é€æ­¥è¾“å‡º**ï¼Œæ”¯æŒå®æ—¶æ’­æ”¾ã€‚

---


### **VITSModel ä½¿ç”¨ç¤ºä¾‹**

#### 1ï¸âƒ£ åˆå§‹åŒ–æ¨¡å‹
æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼åˆå§‹åŒ– `VITSModel`ï¼Œå¹¶åŠ è½½å¿…è¦çš„é…ç½®ä¸é¢„è®­ç»ƒæ¨¡å‹ã€‚

```python
from stream_vits_zh import VITSModel

model = VITSModel(config_path='path/to/config.json', model_path='path/to/vits_bert_model.pth')
```

#### 2ï¸âƒ£ è¿›è¡Œæ¨ç†
ä½¿ç”¨ `inference_stream` æ–¹æ³•è¾“å…¥æ–‡æœ¬å¹¶ç”ŸæˆéŸ³é¢‘ã€‚

```python
txt = "è¿™æ˜¯ä¸€æ®µä¸­æ–‡è¯­éŸ³åˆæˆçš„æµ‹è¯•æ–‡æœ¬ã€‚"
for audio_chunk in model.inference_stream(txt):
    # ä½¿ç”¨ PyAudio æ’­æ”¾éŸ³é¢‘å—
    stream.write(audio_chunk.tobytes())
```

åœ¨ä¸Šè¿°ä»£ç ä¸­ï¼Œ`audio_chunk` æ˜¯ä¸€ä¸ªé€æ­¥ç”Ÿæˆçš„éŸ³é¢‘å—ï¼Œå¯ä»¥å®æ—¶æ’­æ”¾ã€‚

### **å®Œæ•´çš„ VITSModel ç±»å®šä¹‰**:

```python
class VITSModel:
    def __init__(self, config_path: str, model_path: str):
        """
        åˆå§‹åŒ– VITS æ¨¡å‹ï¼ŒåŠ è½½é…ç½®æ–‡ä»¶å’Œé¢„è®­ç»ƒæ¨¡å‹ã€‚
        
        Args:
            config_path (str): é…ç½®æ–‡ä»¶è·¯å¾„ã€‚
            model_path (str): è®­ç»ƒå¥½çš„æ¨¡å‹è·¯å¾„ã€‚
        """
        # åŠ è½½é…ç½®æ–‡ä»¶å’Œæ¨¡å‹
        self.config = utils.get_hparams_from_file(config_path)
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.net_g = self._load_model(model_path)
        
    def _load_model(self, model_path: str):
        """
        åŠ è½½ VITS æ¨¡å‹ã€‚
        
        Args:
            model_path (str): æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚
        
        Returns:
            torch.nn.Module: åŠ è½½çš„æ¨¡å‹ã€‚
        """
        # åŠ è½½æ¨¡å‹
        model_class = utils.load_class(self.config.train.eval_class)
        model = model_class(len(symbols), self.config.data.filter_length // 2 + 1, 
                            self.config.train.segment_size // self.config.data.hop_length, 
                            **self.config.model)
        utils.load_model(model_path, model)
        model.eval()
        model.to(self.device)
        return model

    def inference_stream(self, txt: str):
        """
        æµå¼è¿”å›éŸ³é¢‘å—ã€‚
        
        Args:
            txt (str): è¾“å…¥çš„æ–‡æœ¬ã€‚
        
        Yields:
            np.ndarray: é€æ­¥ç”Ÿæˆçš„éŸ³é¢‘å—ã€‚
        """
        # ç”ŸæˆéŸ³é¢‘å—çš„é€»è¾‘ï¼ˆç•¥ï¼‰
        phonemes, char_embeds = self._text_to_phonemes(txt)
        input_ids = cleaned_text_to_sequence(phonemes)
        buffer_queue = queue.Queue(maxsize=5)
        stop_event = threading.Event()

        def producer():
            with torch.no_grad():
                x_tst = torch.LongTensor(input_ids).unsqueeze(0).to(self.device)
                x_tst_lengths = torch.LongTensor([len(input_ids)]).to(self.device)
                x_tst_prosody = torch.FloatTensor(char_embeds).unsqueeze(0).to(self.device)

                for chunk in self.net_g.inference_stream(x_tst, x_tst_lengths, x_tst_prosody):
                    buffer_queue.put(chunk)
            stop_event.set()

        # å¼€å¯çº¿ç¨‹è¿›è¡Œæ¨ç†
        producer_thread = threading.Thread(target=producer)
        producer_thread.start()

        # æŒç»­è¾“å‡ºéŸ³é¢‘å—
        while not (stop_event.is_set() and buffer_queue.empty()):
            chunk = buffer_queue.get()
            yield chunk
```

---


## **ğŸŒŸ æ¬¢è¿ Star & è´¡çŒ®**

ğŸ’– å¦‚æœä½ è§‰å¾—è¿™ä¸ªé¡¹ç›®æœ‰ç”¨ï¼Œè¯·ç»™ä¸ª â­ **Star** æ”¯æŒï¼

ğŸ“¢ æœŸå¾…ä½ çš„ PR å’Œ Issue åé¦ˆï¼Œå…±åŒä¼˜åŒ– Stream_Vits_Zh ğŸš€

ğŸ“¬ **è”ç³»ä½œè€…ï¼š** [GitHub Issues](https://github.com/zzl410/stream_vits_zh/issues)

---


